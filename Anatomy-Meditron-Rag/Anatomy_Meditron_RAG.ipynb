{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo_x2-4JNw6S"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -r requirements.txt\n",
        "\n",
        "# torch\n",
        "# sentence-transformers==2.2.2\n",
        "# transformers\n",
        "# langchain\n",
        "# fastapi\n",
        "# uvicorn\n",
        "# pypdf\n",
        "# PyPDF2\n",
        "# jinja2\n",
        "# qdrant-client\n",
        "# ctransformers\n",
        "# python-multipart\n",
        "# aiofiles\n",
        "# pdfquery\n",
        "# chromadb\n",
        "# python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader , TextLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pdfquery import PDFQuery\n",
        "import warnings\n",
        "import os\n",
        "from dotenv import load_dotenv, dotenv_values"
      ],
      "metadata": {
        "id": "FBAxnhEXPnbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir data\n",
        "# !cd data && mkdir ChromaDB\n",
        "# !touch requirements.txt\n",
        "# !touch .env"
      ],
      "metadata": {
        "id": "KhhLFV2L3WIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the embedding model\n",
        "embeddings = SentenceTransformerEmbeddings(model_name='NeuML/pubmedbert-base-embeddings')"
      ],
      "metadata": {
        "id": "3H227MANSYsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document loader\n",
        "loader = DirectoryLoader('./data/', glob='**/*.pdf', show_progress=True, loader_cls=PyPDFLoader)"
      ],
      "metadata": {
        "id": "i7slv0fHWDhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load();"
      ],
      "metadata": {
        "id": "rq7FBgc-XOHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the loaded docs into text\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1024,\n",
        "    chunk_overlap = 100,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(documents=documents)"
      ],
      "metadata": {
        "id": "lY3MB5hFXTQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Vector DB\\\n",
        "Supplying a persist_directory stores the embedding on disk"
      ],
      "metadata": {
        "id": "ptOF76idk09m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = './data/ChromaDB'\n",
        "\n",
        "chromadb = Chroma.from_documents(\n",
        "    texts,\n",
        "    embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "metadata": {
        "id": "pA0QmbjKiXUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a Retriever"
      ],
      "metadata": {
        "id": "SIP3oM0to1Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = chromadb.as_retriever(search_kwargs={'k':5})"
      ],
      "metadata": {
        "id": "lhiviekIo5C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a chain"
      ],
      "metadata": {
        "id": "Kbe3Ddfkpzmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_llm = 'TheBloke/meditron-7B-chat-GGUF' ## LLM to use.\n",
        "\n",
        "config = {\n",
        "    'max_new tokens':2000,\n",
        "    'context_length':2048,\n",
        "    'repetition_penalty':1.1,\n",
        "    'temperature': 0.1,\n",
        "    'top_k':50,\n",
        "    'top_p':0.9,\n",
        "    'stream':True,\n",
        "    # 'threads':int(os.cpu_count()/2)\n",
        "}\n",
        "\n",
        "llm = CTransformers(\n",
        "    model=local_llm,\n",
        "    model_type='llama',\n",
        "    **config,\n",
        "\n",
        ")\n",
        "\n",
        "print(llm('AI is going to'))"
      ],
      "metadata": {
        "id": "fnbVFoaGmjHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "  Use the following pieces of information to answer the user's questions.\n",
        "  If the question is related to anatomy and physiology, answer the question appropriately using the knowledge you have.\n",
        "  If the question is is not anatomy or physiology related, just say you don't know the answer.\n",
        "  Do not try to make up answers in case you do not know the answer.\n",
        "\n",
        "  Context: {context}\n",
        "  Question: {question}\n",
        "\n",
        "  Return the helpful answer below, nothing else.\n",
        "  Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ],
      "metadata": {
        "id": "qhVtvtWiv_QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the qa_chain**"
      ],
      "metadata": {
        "id": "qoisoATr-sQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", message=\"Number of tokens \\(\\d+\\) exceeded maximum context length \\(512\\)\", category=UserWarning)\n",
        "question = \"Central nervous system?\"\n",
        "result = qa_chain({\"query\": question});\n",
        "# Check the result of the query\n",
        "# result[\"result\"];\n",
        "# Check the source document from where we\n",
        "result[\"source_documents\"][0]\n"
      ],
      "metadata": {
        "id": "waRp79X6-q1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHDry3EMc81t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}